---
title: "ML Robustness"
description: "Verifying Properties of Machine Learning"
problem_statement: "Give theoretical guarantees for ML robustness"
contribution: "We designed a HiDra attack to show current robust aggregators are impractical. And ..."
---

How can we know if a ML model is robust, biased or susceptible to a poisoning attack? Can we derive statistically sound answers to such questions? We show that we can do so with two approaches for several security applications. What about properties of the training process (i.e., stochastic gradient descent) such as being able to obtain the same model update from two different sets of samples?