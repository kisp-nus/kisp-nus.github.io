<!DOCTYPE html>
<html lang="en"><head>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Maps | NUS KISP Lab</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Maps" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Machine learning and Algorithms for Practical Security" />
<meta property="og:description" content="Machine learning and Algorithms for Practical Security" />
<link rel="canonical" href="http://localhost:4000/projects/maps" />
<meta property="og:url" content="http://localhost:4000/projects/maps" />
<meta property="og:site_name" content="NUS KISP Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-02T23:23:40+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Maps" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-02T23:23:40+08:00","datePublished":"2025-03-02T23:23:40+08:00","description":"Machine learning and Algorithms for Practical Security","headline":"Maps","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/projects/maps"},"url":"http://localhost:4000/projects/maps"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="NUS KISP Lab" /><link rel="stylesheet" href="/css/style.css">


</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">NUS KISP Lab</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/people/">People</a><a class="page-link" href="/projects/">Projects</a><a class="page-link" href="/pubs/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        

<article class="post">

  <header class="post-header">
    <div class="project-title">
      <h1 class="post-title">MAPS</h1>
      <h2> Machine learning and Algorithms for Practical Security </h2>
    </div>
    <div class="project-image">
        <img src="/images/projects/maps_small.png" alt="Project maps logo"/>
      </div>

      <div class="proj-team">
        Project team:
  
  <a href="/people/teo">Teodora Baluta</a>,


  
  <a href="/people/aashish">Aashish Kolluri</a>,


  
  <a href="/people/kareem">Kareem Shehata</a>,


  
  <a href="/people/ivica">Ivica Nikolić</a>

</div>
  </header>

  <div class="post-content"><h3>Research Overview</h3>
<hr />

<p>Security of machine learning (ML) systems is a relatively new topic in computer security with many concerns highlighted in the last decade through attack procedures and defenses.
However, fundamental questions in this space such as distinguishing between the intended vs. unintended (malicious) behavior or developing algorithms to test and verify desirable properties with guarantees remain open.
This is because, among others, ML models are learned through complex stochastic procedures from data coming from an unknown distribution, we often do not have ground truth of what they should be learning, and it is difficult to perfectly capture the highly non-linear and high-dimensional internals of the models.
On top of all this, the adversaries can be adaptive or employ their own ML strategies.</p>

<p>It is thus imperative to analyze security in a <em>systematic</em> way when considering machine learning systems in <em>practical</em> setups.</p>

<p>The <strong>MAPS</strong> (<strong>M</strong>achine learning and <strong>A</strong>lgorithms for <strong>P</strong>ractical <strong>S</strong>ecurity) project aims to design and develop (1) novel machine learning algorithms that come with strong security &amp; privacy guarantees and (2) rigorous analysis of security properties of off-the-shelf machine learning systems.</p>

<p>At a technical level, we work on three directions:</p>
<ol>
  <li><strong>Developing Algorithms with Guarantees</strong>
    <ul>
      <li>Learning in federated setups enables more control over their data for users. Despite this advantage, it is difficult to ensure that the privacy of users is preserved with strong guarantees while learning something useful from the users’ data. In this line of works, we focus on the question if there exist algorithms that show a good trade-off between utility and privacy. We show several key technical contributions that build on top of the principled framework of <strong>differential privacy</strong> to answer positively. Relevant projects are in [ML Privacy].</li>
    </ul>
  </li>
  <li><strong>Verifying Properties of Machine Learning</strong>
    <ul>
      <li>How can we know if a ML model is robust, biased or susceptible to a poisoning attack? Can we derive statistically sound answers to such questions? We show that we can do so with two approaches for several security applications. What about properties of the training process (i.e., stochastic gradient descent) such as being able to obtain the same model update from two different sets of samples?</li>
    </ul>
  </li>
  <li><strong>Learning Algorithms for Programs</strong>
    <ul>
      <li>Can we learn from observations rules that are useful for security analysis such as taint analysis rules? In this line of work, we use ML to power analyses useful for security analysis.</li>
    </ul>
  </li>
</ol>

<hr />
</div>
  <div class="header">
      <div class="links">
        <a class="categoryButton big-btn btn-sm z-depth-0" data-category="ML Privacy" role="button">ML Privacy</a>
      </div>
      <div class="links">
        <a class="categoryButton big-btn btn-sm z-depth-0" data-category="ML Robustness" role="button">ML Robustness</a>
      </div>
      <div class="links">
        <a class="categoryButton big-btn btn-sm z-depth-0" data-category="ML Applications" role="button">ML Applications</a>
      </div></div>
  <div class="category-section" data-category="ML Privacy">
    <h2 class="category">ML Privacy</h2>
    <div class="container">
      <div class="row row-cols-2"><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Unforgeability in Stochastic Gradient Descent</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/teo">Teodora Baluta</a>,


  
  <a href="/people/ivica">Ivica Nikolić</a>,


  Racchit Jain,


  <a href="https://sites.google.com/site/diveshhomepage">Divesh Aggarwal</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">CCS 2023</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="https://github.com/teobaluta/unforgeability-SGD" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Stochastic Gradient Descent (SGD) is a popular training algorithm, a cornerstone of modern machine learning systems. Several security applications benefit from determining if SGD executions are forgeable, i.e., whether the model parameters seen at a given step are obtainable by more than one distinct set of data samples. In this paper, we present the first attempt at proving impossibility of such forgery. We furnish a set of conditions, which are efficiently checkable on concrete checkpoints seen during training runs, under which checkpoints are provably unforgeable at that step. Our experiments show that the conditions are somewhat mild and hence always satisfied at checkpoints sampled in our experiments. Our results sharply contrast prior findings at a high level: We show that checkpoints we find to be provably unforgeable have been deemed to be forgeable using the same methodology and experimental setup suggested in prior work. This discrepancy arises because of unspecified subtleties in definitions. We experimentally confirm that the distinction matters, i.e., small errors amplify during training to produce significantly observable difference in final models trained. We hope our results serve as a cautionary note on the role of algebraic precision in forgery definitions and related security arguments.</p>
    
  </div>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">LPGNet: Link Private Graph Networks for Node Classification</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/aashish">Aashish Kolluri</a>,


  
  <a href="/people/teo">Teodora Baluta</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">CCS 2022</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Classification tasks on labeled graph-structured data have many important applications ranging from social recommendation to financial modeling. Deep neural networks are increasingly being used for node classification on graphs, wherein nodes with similar features have to be given the same label. Graph convolutional networks (GCNs) are one such widely studied neural network architecture that perform well on this task. However, powerful link-stealing attacks on GCNs have recently shown that even with black-box access to the trained model, inferring which links (or edges) are present in the training graph is practical. In this paper, we present a new neural network architecture called LPGNet for training on graphs with privacy-sensitive edges. LPGNet provides differential privacy (DP) guarantees for edges using a novel design for how graph edge structure is used during training. We empirically show that LPGNet models often lie in the sweet spot between providing privacy and utility: They can offer better utility than "trivially" private architectures which use no edge information (e.g., vanilla MLPs) and better resilience against existing link-stealing attacks than vanilla GCNs which use the full edge structure. LPGNet also offers consistently better privacy-utility tradeoffs than DPGCN, which is the state-of-the-art mechanism for retrofitting differential privacy into conventional GCNs, in most of our evaluated datasets.</p>
    
  </div>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Membership Inference Attacks and Generalization: A Causal Perspective</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/teo">Teodora Baluta</a>,


  Shiqi Shen,


  S. Hitarth,


  <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">CCS 2022</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="https://github.com/teobaluta/etio" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Membership inference (MI) attacks highlight a privacy weakness in present stochastic training methods for neural networks. It is not well understood, however, why they arise. Are they a natural consequence of imperfect generalization only? Which underlying causes should we address during training to mitigate these attacks? Towards answering such questions, we propose the first approach to explain MI attacks and their connection to generalization based on principled causal reasoning. We offer causal graphs that quantitatively explain the observed MI attack performance achieved for 6 attack variants. We refute several prior non-quantitative hypotheses that over-simplify or over-estimate the influence of underlying causes, thereby failing to capture the complex interplay between several factors. Our causal models also show a new connection between generalization and MI attacks via their shared causal factors. Our causal models have high predictive power (0.90), i.e., their analytical predictions match with observations in unseen experiments often, which makes analysis via them a pragmatic alternative.</p>
    
  </div>
  
  <p class="video"><iframe class="video" src="https://www.youtube.com/embed/0GixTGzbTtE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Private Hierarchical Clustering in Federated Networks</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/aashish">Aashish Kolluri</a>,


  
  <a href="/people/teo">Teodora Baluta</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">CCS 2021</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Analyzing structural properties of social networks, such as identifying their clusters or finding their most central nodes, has many applications. However, these applications are not supported by federated social networks that allow users to store their social links locally on their end devices. In the federated regime, users want access to personalized services while also keeping their social links private. In this paper, we take a step towards enabling analytics on federated networks with differential privacy guarantees about protecting the user links or contacts in the network. Specifically, we present the first work to compute hierarchical cluster trees using local differential privacy. Our algorithms for computing them are novel and come with theoretical bounds on the quality of the trees learned. The private hierarchical cluster trees enable a service provider to query the community structure around a user at various granularities without the users having to share their raw contacts with the provider. We demonstrate the utility of such queries by redesigning the state-of-the-art social recommendation algorithms for the federated setup. Our recommendation algorithms significantly outperform the baselines which do not use social contacts and are on par with the non-private algorithms that use contacts.</p>
    
  </div>
  
</div>
      </div>
    </div>
    </div>
  
  <div class="category-section" data-category="ML Robustness">
    <h2 class="category">ML Robustness</h2>
    <div class="container">
      <div class="row row-cols-2"><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Attacking Byzantine Robust Aggregation in High Dimensions</div>
      <div class="pub-authors">
        
        
  Sarthak Choudhary,


  
  <a href="/people/aashish">Aashish Kolluri</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">S&P 2024</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Training modern neural networks or models typically requires averaging over a sample of high-dimensional vectors. Poisoning attacks can skew or bias the average vectors used to train the model, forcing the model to learn specific patterns or avoid learning anything useful. Byzantine robust aggregation is a principled algorithmic defense against such biasing. Robust aggregators can bound the maximum bias in computing centrality statistics, such as mean, even when some fraction of inputs are arbitrarily corrupted. Designing such aggregators is challenging when dealing with high dimensions. However, the first polynomial-time algorithms with strong theoretical bounds on the bias have recently been proposed. Their bounds are independent of the number of dimensions, promising a conceptual limit on the power of poisoning attacks in their ongoing arms race against defenses. In this paper, we show a new attack called HIDRA on practical realization of strong defenses which subverts their claim of dimension-independent bias. HIDRA highlights a novel computational bottleneck that has not been a concern of prior information-theoretic analysis. Our experimental evaluation shows that our attacks almost completely destroy the model performance, whereas existing attacks with the same goal fail to have much effect. Our findings leave the arms race between poisoning attacks and provable defenses wide open.</p>
    
  </div>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Scalable Quantitative Verification For Deep Neural Networks</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/teo">Teodora Baluta</a>,


  Zheng Leong Chua,


  <a href="https://www.cs.toronto.edu/~meel/">Kuldeep S. Meel</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">ICSE 2021</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="https://github.com/teobaluta/provero" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Despite the functional success of deep neural networks (DNNs), their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep neural networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep neural networks, with provable guarantees, even where testers only have black-box access to the neural network.</p>
    
  </div>
  
  <p class="video"><iframe class="video" src="https://www.youtube.com/embed/Hc-Xjdf2LNc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">Quantitative verification of neural networks and its security applications</div>
      <div class="pub-authors">
        
        
  
  <a href="/people/teo">Teodora Baluta</a>,


  Shiqi Shen,


  <a href="https://n.ethz.ch/~sshivaji/">Shweta Shinde</a>,


  <a href="https://www.cs.toronto.edu/~meel/">Kuldeep S. Meel</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">CCS 2019</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="https://github.com/teobaluta/NPAQ" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Neural networks are increasingly employed in safety-critical domains. This has prompted interest in verifying or certifying logically encoded properties of neural networks. Prior work has largely focused on checking existential properties, wherein the goal is to check whether there exists any input that violates a given property of interest. However, neural network training is a stochastic process, and many questions arising in their analysis require probabilistic and quantitative reasoning, i.e., estimating how many inputs satisfy a given property. To this end, our paper proposes a novel and principled framework to quantitative verification of logical properties specified over neural networks. Our framework is the first to provide PAC-style soundness guarantees, in that its quantitative estimates are within a controllable and bounded error from the true count. We instantiate our algorithmic framework by building a prototype tool called NPAQ that enables checking rich properties over binarized neural networks. We show how emerging security analyses can utilize our framework in 3 concrete point applications: quantifying robustness to adversarial inputs, efficacy of trojan attacks, and fairness/bias of given neural networks.</p>
    
  </div>
  
  <p class="video"><iframe class="video" src="https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3ZjllNjdhNzE3ZDY5YzMwODFmZGNiZGRkMjYwZTQ3OSIsImV4cCI6MTcxNzc3NDA1Nywia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.BlADv_nZEXCOAp2qX40cW21NgORulYBtefrJ92jyQ5tvawGHm7O8u75_6ZX-L8Y-v9nahehGVVf684g_ISGLoIKBV2SZ6n9OQKzldGX_KWjlNjCSeu3SxGuZNQ_SHOcMXQitE0HbdLNrGm4hYI0Q0T5mSSnbPG3RdLVZmCYrpRlKmrXWzeYMoqYb0Z7X4nbK4FXHs-4W7770QBaeAw7Ryybyamhde9C_fD-etsrC1MdeKThndAcmszej4_5j9L_FFMWVOrQf1BrRcIX936WmfjYRpQHXqAgZuYDyGsafXrcbPvxLAKW5SXHcbOPD6IN3w7lUPe9qGEB8EcGujxPgvg?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3ZjllNjdhNzE3ZDY5YzMwODFmZGNiZGRkMjYwZTQ3OSIsImV4cCI6MTcxNzc3NDA1Nywia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.BlADv_nZEXCOAp2qX40cW21NgORulYBtefrJ92jyQ5tvawGHm7O8u75_6ZX-L8Y-v9nahehGVVf684g_ISGLoIKBV2SZ6n9OQKzldGX_KWjlNjCSeu3SxGuZNQ_SHOcMXQitE0HbdLNrGm4hYI0Q0T5mSSnbPG3RdLVZmCYrpRlKmrXWzeYMoqYb0Z7X4nbK4FXHs-4W7770QBaeAw7Ryybyamhde9C_fD-etsrC1MdeKThndAcmszej4_5j9L_FFMWVOrQf1BrRcIX936WmfjYRpQHXqAgZuYDyGsafXrcbPvxLAKW5SXHcbOPD6IN3w7lUPe9qGEB8EcGujxPgvg%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
  
</div><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">AUROR: Defending Against Poisoning Attacks in Collaborative Deep Learning Systems</div>
      <div class="pub-authors">
        
        
  Shiqi Shen,


  <a href="https://www.microsoft.com/en-us/research/people/shtople/">Shruti Tople</a>,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>


      </div>
      <div class="pub-info">ACSAC 2016</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Deep learning in a collaborative setting is emerging as a cornerstone of many upcoming applications, wherein untrusted users collaborate to generate more accurate models. From the security perspective, this opens collaborative deep learning to poisoning attacks, wherein adversarial users deliberately alter their inputs to mis-train the model. These attacks are known for machine learning systems in general, but their impact on new deep learning systems is not well-established. We investigate the setting of indirect collaborative deep learning — a form of practical deep learning wherein users submit masked features rather than direct data. Indirect collaborative deep learning is preferred over direct, because it distributes the cost of computation and can be made privacy-preserving. In this paper, we study the susceptibility of collaborative deep learning systems to adversarial poisoning attacks. Specifically, we obtain the following empirical results on 2 popular datasets for handwritten images (MNIST) and traffic signs (GTSRB) used in auto-driving cars. For collaborative deep learning systems, we demonstrate that the attacks have 99% success rate for misclassifying specific target data while poisoning only 10% of the entire training dataset. As a defense, we propose AUROR, a system that detects malicious users and generates an accurate model. The accuracy under the deployed defense on practical datasets is nearly unchanged when operating in the absence of attacks. The accuracy of a model trained using AUROR drops by only 3% even when 30% of all the users are adversarial. AUROR provides a strong guarantee against evasion; if the attacker tries to evade, its attack effectiveness is bounded.</p>
    
  </div>
  
</div>
      </div>
    </div>
    </div>
  
  <div class="category-section" data-category="ML Applications">
    <h2 class="category">ML Applications</h2>
    <div class="container">
      <div class="row row-cols-2"><div class="pub-container">
  <div class="toggle-container">
    <div class="links">
      <div class="pub-title">One Engine To Serve 'em All: Inferring Taint Rules Without Architectural Semantics</div>
      <div class="pub-authors">
        
        
  Zheng Leong Chua,


  
  <a href="/people/teo">Teodora Baluta</a>,


  Yanhao Wang,


  <a href="https://www.comp.nus.edu.sg/~prateeks/">Prateek Saxena</a>,


  Zhenkai Liang,


  Purui Su


      </div>
      <div class="pub-info">NDSS 2019</div>
      <div class="button-container">
        <a class="toggleButton btn btn-sm z-depth-0" role="button">Abstract</a>
        <a href="" class="btn btn-sm z-depth-0" role="button">Code</a>
        <a class="videoButton btn btn-sm z-depth-0" role="button">Video</a>
      </div>
    </div>
    
    <p class="text">Dynamic binary taint analysis has wide applications in the security analysis of commercial-off-the-shelf (COTS) binaries. One of the key challenges in dynamic binary analysis is to specify the taint rules that capture how taint information propagates for each instruction on an architecture. Most of the existing solutions specify taint rules using a deductive approach by summarizing the rules manually after analyzing the instruction semantics. Intuitively, taint propagation reflects on how an instruction input affects its output and thus can be observed from instruction executions. In this work, we propose an inductive method for taint propagation and develop a universal taint tracking engine that is architecture-agnostic. Our taint engine, TAINTINDUCE, can learn taint rules with minimal architectural knowledge by observing the execution behavior of instructions. To measure its correctness and guide taint rule generation, we define the precise notion of soundness for bit-level taint tracking in this novel setup. In our evaluation, we show that TAINT INDUCE automatically learns rules for 4 widely used architectures: x86, x64, AArch64, and MIPS-I. It can detect vulnerabilities for 24 CVEs in 15 applications on both Linux and Windows over millions of instructions and is comparable with other mature existing tools (TEMU, libdft, Triton). TAINTINDUCE can be used as a standalone taint engine or be used to complement existing taint engines for unhandled instructions. Further, it can be used as a cross-referencing tool to uncover bugs in taint engines, emulation implementations and ISA documentations. significantly empowers the attacker.</p>
    
  </div>
  
</div>
      </div>
    </div>
    </div>
  
</article>


<script>
  $(document).ready(function() {
      $('.toggleButton').click(function() {
          var container = $(this).closest('.pub-container');
          var text = container.find('.text');
          text.toggle();
          text.toggleClass('boxed');
      });

      $('.videoButton').click(function() {
          var container = $(this).closest('.pub-container');
          var video = container.find('.video');
          video.toggle();
          video.toggleClass('video-show');
      });
      $('.categoryButton').click(function() {
          var category = $(this).data('category');
          var section = $('.category-section[data-category="' + category + '"]');

          // Toggle the visibility of the selected category section
          section.toggle();

          // Hide all other category sections
          $('.category-section').not(section).hide();
      });

  });
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">NUS KISP Lab</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">NUS KISP Lab</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Keep It Secure and Private</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
